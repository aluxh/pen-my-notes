{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"notes/aws-sdk/","text":"AWS SDK The following is to demonstrate the infrastructure as code. Creating Redshift Cluster using the AWS python SDK Few libraries you need are pandas , boto3 (AWS SDK), json packages. And, the steps are as follows: Prepare the params and environment Using an external cfg file to store all the params, and load into notebook using configparser . How the config file looks like: [AWS] KEY=key SECRET=secret [DWH] DWH_CLUSTER_TYPE=multi-node DWH_NUM_NODES=4 DWH_NODE_TYPE=dc2.large DWH_IAM_ROLE_NAME=dwhRole DWH_CLUSTER_IDENTIFIER=dwhCluster DWH_DB=dwh DWH_DB_USER=dwhuser DWH_DB_PASSWORD=Passw0rd DWH_PORT=5439 How the python script looks like to load into the notebook: import configparser config = configparser.ConfigParser() config.read_file(open('dwh.cfg')) KEY = config.get('AWS','KEY') SECRET = config.get('AWS','SECRET') DWH_CLUSTER_TYPE = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\") DWH_NUM_NODES = config.get(\"DWH\",\"DWH_NUM_NODES\") DWH_NODE_TYPE = config.get(\"DWH\",\"DWH_NODE_TYPE\") DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\") DWH_DB = config.get(\"DWH\",\"DWH_DB\") DWH_DB_USER = config.get(\"DWH\",\"DWH_DB_USER\") DWH_DB_PASSWORD = config.get(\"DWH\",\"DWH_DB_PASSWORD\") DWH_PORT = config.get(\"DWH\",\"DWH_PORT\") DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\") (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB) pd.DataFrame({\"Param\": [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"], \"Value\": [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME] }) Create the resources and clients for EC2, S3, IAM and Redshift Using boto3 library, create resources and clients in AWS. import boto3 ec2 = boto3.resource('ec2', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) s3 = boto3.resource('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) iam = boto3.client('iam', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) redshift = boto3.client('redshift', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) Try printing out sample data sources from s3 using the following template: sampleDbBucket = s3.Bucket(\"bucket-name\") for obj in sampleDbBucket.objects.filter(Prefix=\"some prefix to filter data\"): print (obj) Create Role and Attach Policy Creating IAM role to allow AWS Redshift to call AWS services on your behalf. # Create Role try: print('1.1 Creating a new IAM Role') dwhRole = iam.create_role( Path='/', RoleName=DWH_IAM_ROLE_NAME, Description=\"Allows Redshift clusters to call AWS services on your behalf.\", AssumeRolePolicyDocument=json.dumps( { 'Statement': [{'Action': 'sts:AssumeRole', 'Effect': 'Allow', 'Principal': {'Service': 'redshift.amazonaws.com'}}], 'Version': '2012-10-17'} ) ) except Exception as e: print(e) # Attach Policy print('1.2 Attaching Policy') iam.attach_role_policy( RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" )['ResponseMetadata']['HTTPStatusCode'] # Get and print the IAM role ARN print('1.3 Get the IAM role ARN') roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn'] print(roleArn) Create Redshift Cluster Creating the redshift cluster using the standard template. try: response = redshift.create_cluster( #HW ClusterType=DWH_CLUSTER_TYPE, NodeType=DWH_NODE_TYPE, NumberOfNodes=int(DWH_NUM_NODES), #Identifiers & Credentials DBName=DWH_DB, ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, MasterUsername=DWH_DB_USER, MasterUserPassword=DWH_DB_PASSWORD, #Roles (for s3 access) IamRoles=[roleArn] ) except Exception as e: print(e) Then, describe the cluster to examine the parameters and its status. Run this block several times until the cluster status becomes Available def prettyRedshiftProps(props): pd.set_option('display.max_colwidth', -1) keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId'] x = [(k, v) for k,v in props.items() if k in keysToShow] return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"]) myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] prettyRedshiftProps(myClusterProps) Print out the cluster endpoint. Only run this when the cluster is available DWH_ENDPOINT = myClusterProps['Endpoint']['Address'] DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn'] print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT) print(\"DWH_ROLE_ARN :: \", roleArn) Open an incoming TCP port to access the cluster endpoint try: vpc = ec2.Vpc(id=myClusterProps['VpcId']) defaultSg = list(vpc.security_groups.all())[0] print(defaultSg) defaultSg.authorize_ingress( GroupName=defaultSg.group_name, CidrIp='0.0.0.0/0', IpProtocol='TCP', FromPort=int(DWH_PORT), ToPort=int(DWH_PORT) ) except Exception as e: print(e) Test that you can access the cluster %load_ext sql conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB) print(conn_string) %sql $conn_string","title":"AWS SDK"},{"location":"notes/aws-sdk/#aws-sdk","text":"The following is to demonstrate the infrastructure as code.","title":"AWS SDK"},{"location":"notes/aws-sdk/#creating-redshift-cluster-using-the-aws-python-sdk","text":"Few libraries you need are pandas , boto3 (AWS SDK), json packages. And, the steps are as follows:","title":"Creating Redshift Cluster using the AWS python SDK"},{"location":"notes/aws-sdk/#prepare-the-params-and-environment","text":"Using an external cfg file to store all the params, and load into notebook using configparser . How the config file looks like: [AWS] KEY=key SECRET=secret [DWH] DWH_CLUSTER_TYPE=multi-node DWH_NUM_NODES=4 DWH_NODE_TYPE=dc2.large DWH_IAM_ROLE_NAME=dwhRole DWH_CLUSTER_IDENTIFIER=dwhCluster DWH_DB=dwh DWH_DB_USER=dwhuser DWH_DB_PASSWORD=Passw0rd DWH_PORT=5439 How the python script looks like to load into the notebook: import configparser config = configparser.ConfigParser() config.read_file(open('dwh.cfg')) KEY = config.get('AWS','KEY') SECRET = config.get('AWS','SECRET') DWH_CLUSTER_TYPE = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\") DWH_NUM_NODES = config.get(\"DWH\",\"DWH_NUM_NODES\") DWH_NODE_TYPE = config.get(\"DWH\",\"DWH_NODE_TYPE\") DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\") DWH_DB = config.get(\"DWH\",\"DWH_DB\") DWH_DB_USER = config.get(\"DWH\",\"DWH_DB_USER\") DWH_DB_PASSWORD = config.get(\"DWH\",\"DWH_DB_PASSWORD\") DWH_PORT = config.get(\"DWH\",\"DWH_PORT\") DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\") (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB) pd.DataFrame({\"Param\": [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"], \"Value\": [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME] }) Create the resources and clients for EC2, S3, IAM and Redshift Using boto3 library, create resources and clients in AWS. import boto3 ec2 = boto3.resource('ec2', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) s3 = boto3.resource('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) iam = boto3.client('iam', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) redshift = boto3.client('redshift', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) Try printing out sample data sources from s3 using the following template: sampleDbBucket = s3.Bucket(\"bucket-name\") for obj in sampleDbBucket.objects.filter(Prefix=\"some prefix to filter data\"): print (obj)","title":"Prepare the params and environment"},{"location":"notes/aws-sdk/#create-role-and-attach-policy","text":"Creating IAM role to allow AWS Redshift to call AWS services on your behalf. # Create Role try: print('1.1 Creating a new IAM Role') dwhRole = iam.create_role( Path='/', RoleName=DWH_IAM_ROLE_NAME, Description=\"Allows Redshift clusters to call AWS services on your behalf.\", AssumeRolePolicyDocument=json.dumps( { 'Statement': [{'Action': 'sts:AssumeRole', 'Effect': 'Allow', 'Principal': {'Service': 'redshift.amazonaws.com'}}], 'Version': '2012-10-17'} ) ) except Exception as e: print(e) # Attach Policy print('1.2 Attaching Policy') iam.attach_role_policy( RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" )['ResponseMetadata']['HTTPStatusCode'] # Get and print the IAM role ARN print('1.3 Get the IAM role ARN') roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn'] print(roleArn)","title":"Create Role and Attach Policy"},{"location":"notes/aws-sdk/#create-redshift-cluster","text":"Creating the redshift cluster using the standard template. try: response = redshift.create_cluster( #HW ClusterType=DWH_CLUSTER_TYPE, NodeType=DWH_NODE_TYPE, NumberOfNodes=int(DWH_NUM_NODES), #Identifiers & Credentials DBName=DWH_DB, ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, MasterUsername=DWH_DB_USER, MasterUserPassword=DWH_DB_PASSWORD, #Roles (for s3 access) IamRoles=[roleArn] ) except Exception as e: print(e) Then, describe the cluster to examine the parameters and its status. Run this block several times until the cluster status becomes Available def prettyRedshiftProps(props): pd.set_option('display.max_colwidth', -1) keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId'] x = [(k, v) for k,v in props.items() if k in keysToShow] return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"]) myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] prettyRedshiftProps(myClusterProps) Print out the cluster endpoint. Only run this when the cluster is available DWH_ENDPOINT = myClusterProps['Endpoint']['Address'] DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn'] print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT) print(\"DWH_ROLE_ARN :: \", roleArn)","title":"Create Redshift Cluster"},{"location":"notes/aws-sdk/#open-an-incoming-tcp-port-to-access-the-cluster-endpoint","text":"try: vpc = ec2.Vpc(id=myClusterProps['VpcId']) defaultSg = list(vpc.security_groups.all())[0] print(defaultSg) defaultSg.authorize_ingress( GroupName=defaultSg.group_name, CidrIp='0.0.0.0/0', IpProtocol='TCP', FromPort=int(DWH_PORT), ToPort=int(DWH_PORT) ) except Exception as e: print(e) Test that you can access the cluster %load_ext sql conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB) print(conn_string) %sql $conn_string","title":"Open an incoming TCP port to access the cluster endpoint"},{"location":"notes/cassandra/","text":"","title":"Cassandra"},{"location":"notes/etl/","text":"Data Warehouse Topic ETL architecture Data Ingestion Ingestion at scale using the COPY command. Inserting row by row is too slow. If the file is huge, Splitting the file into multiple files Ingest in parallel Using common prefix, or Using a manifest file Other consideration Better to ingest from the same AWS region Better to compress all the CSV files Using Redshift - Optimizing the table design Redshift clusters have multiple nodes, so you can optimize the design of the table to ingest your data, to improve the performance. When a table is partitioned up into many pieces and distributed across slices in different machines, this is done blindly. If one has an idea about the frequent access pattern of a table, one can choose a cleverer strategy. There are 2 possible strategies: Distribution Style Sorting Key Distribution style EVEN distribution With an EVEN key distribution style, a table is partitioned on slices such that each slice would have an almost equal number of records from the partitioned table. (round-robin method) Joining 2 tables distributed using an EVEN stategy is slow because records will be shuffled for putting together the join result. ALL distribution Small tables could be replicated on all slices to speed up joins. And, these are used frequently for dimension tables. (a.k.a broadcasting) AUTO distribution This strategy leave the decision to Redshift to decide. In general, this is what they do: \"Small enough\" tables are distributed with an ALL distribution strategy, while Large tables are distributed with EVEN strategy. KEY distribution Rows with similar keys are put in the same slice. But, this can lead to a skewed distribution, if some dist keys are more frequent than others. However, when a dimension table is too huge to be distributed across ALL strategy, one can organize the distribution for both fact table and dimension table using the same dist key. (Useful trick) Therefore, Redshift collocates the rows from both table one the same slice, using the joining keys. Sorting Key One can define its columns as sort key. Then, upon loading, rows are sorted before distribution to slices. This minimizes the query time since each node already has contiguous ranges of rows based on the sorting key. This is useful for columns that are frequently used in sorting like date dimension (SQL query by ) and, its corresponding foreign key in the fact table.","title":"Data Warehouse Topic"},{"location":"notes/etl/#data-warehouse-topic","text":"","title":"Data Warehouse Topic"},{"location":"notes/etl/#etl-architecture","text":"","title":"ETL architecture"},{"location":"notes/etl/#data-ingestion","text":"Ingestion at scale using the COPY command. Inserting row by row is too slow. If the file is huge, Splitting the file into multiple files Ingest in parallel Using common prefix, or Using a manifest file Other consideration Better to ingest from the same AWS region Better to compress all the CSV files","title":"Data Ingestion"},{"location":"notes/etl/#using-redshift-optimizing-the-table-design","text":"Redshift clusters have multiple nodes, so you can optimize the design of the table to ingest your data, to improve the performance. When a table is partitioned up into many pieces and distributed across slices in different machines, this is done blindly. If one has an idea about the frequent access pattern of a table, one can choose a cleverer strategy. There are 2 possible strategies: Distribution Style Sorting Key","title":"Using Redshift - Optimizing the table design"},{"location":"notes/etl/#distribution-style","text":"EVEN distribution With an EVEN key distribution style, a table is partitioned on slices such that each slice would have an almost equal number of records from the partitioned table. (round-robin method) Joining 2 tables distributed using an EVEN stategy is slow because records will be shuffled for putting together the join result. ALL distribution Small tables could be replicated on all slices to speed up joins. And, these are used frequently for dimension tables. (a.k.a broadcasting) AUTO distribution This strategy leave the decision to Redshift to decide. In general, this is what they do: \"Small enough\" tables are distributed with an ALL distribution strategy, while Large tables are distributed with EVEN strategy. KEY distribution Rows with similar keys are put in the same slice. But, this can lead to a skewed distribution, if some dist keys are more frequent than others. However, when a dimension table is too huge to be distributed across ALL strategy, one can organize the distribution for both fact table and dimension table using the same dist key. (Useful trick) Therefore, Redshift collocates the rows from both table one the same slice, using the joining keys. Sorting Key One can define its columns as sort key. Then, upon loading, rows are sorted before distribution to slices. This minimizes the query time since each node already has contiguous ranges of rows based on the sorting key. This is useful for columns that are frequently used in sorting like date dimension (SQL query by ) and, its corresponding foreign key in the fact table.","title":"Distribution style"},{"location":"notes/postgresql/","text":"Notes from Udacity DEND for PostgreSQL Using Jyupter Notebook to run SQL You can use ipython-sql library. And, load it into the Jyupter notebook using %load_ext sql . To execute SQL queries, you write on of the following at the top of your cell. - %sql : - For a one-liner SQL query - You can access a python variable using $ - %%SQL : - For a multi-line SQL query - You can NOT access a python variable using $ Running a connection like: postgresql://postgres:postgres@db:5432/pagila to connect to the database. Creating a database and fill it with data Adding ! at the beginning at the Jyupter cell runs a command in shell. For example, we're not running python code but we are running the createdb and psql postgresql command-line utilities. An example: !PGPASSWORD=student createdb -h 127.0.0.1 -U student pagila !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-schema.sql !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-data.sql Connect to a database Load the ipython-sql, create the connection variable, then connect to database. # Load ipython-sql library %load_ext sql # Connect to database DB_ENDPOINT = '127.0.0.1' DB = 'pagila' DB_USER = 'student' DB_PASSWORD = 'student' DB_PORT = '5432' # Creating the connection string and keep in python variable: conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, DB_PORT, DB) # Using the python variable %sql $conn_string Retrieve the database schema %%sql SELECT column_name, data_type FROM information_schema.columns WHERE table_name = \"dimdate'; Create FACT table with foreign keys using REFERENCES constraint %%sql CREATE TABLE factSales ( sales_key SERIAL PRIMARY KEY, date_key integer REFERENCES dimdate (date_key), customer_key integer REFERENCES dimcustomer (customer_key), movie_key integer REFERENCES dimmovie (movie_key), store_key integer REFERENCES dimstore(store_key), sales_amount numeric ); Drop tables that contain foreign keys You need to drop the table that has foreign keys references before you can drop other tables. %%sql DROP TABLE IF EXISTS factsales; DROP TABLE IF EXISTS dimdate; DROP TABLE IF EXISTS dimstore; DROP TABLE IF EXISTS dimmovie; DROP TABLE IF EXISTS dimcustomer; ETL process from one database (Normalized) to new tables (Star Schema) An example to extract date data from a payment_date table, then transform and load into the dimDate table %%sql INSERT INTO dimDate (date_key, date, year, quarter, month, day, week, is_weekend) SELECT DISTINCT(TO_CHAR(payment_date :: DATE, 'yyyyMMDD')::integer) AS date_key, date(payment_date) AS date, EXTRACT(year FROM payment_date) AS year, EXTRACT(quarter FROM payment_date) AS quarter, EXTRACT(month FROM payment_date) AS month, EXTRACT(day FROM payment_date) AS day, EXTRACT(week FROM payment_date) AS week, CASE WHEN EXTRACT(ISODOW FROM payment_date) IN (6, 7) THEN true ELSE false END AS is_weekend FROM payment; Check the performance of the script Add %%time at the top of the execution block. An example, %%time %%sql SELECT dimMovie.title, dimDate.month, dimCustomer.city, sum(sales_amount) AS revenue FROM factSales JOIN dimMovie ON (dimMovie.movie_key = factSales.movie_key) JOIN dimDate ON (dimDate.date_key = factSales.date_key) JOIN dimCustomer ON (dimCustomer.customer_key = factSales.customer_key) GROUP BY (dimMovie.title, dimDate.month, dimCustomer.city) ORDER BY dimMovie.title, dimDate.month, dimCustomer.city, revenue DESC; Using columnar storage extension in PostgreSQL The extension is cstore_fdw by citus_data: github link An example: %%sql -- load extension first time after install CREATE EXTENSION cstore_fdw; -- create server object CREATE SERVER cstore_server FOREIGN DATA WRAPPER cstore_fdw; -- create foreign table, i.e. customer reviews column DROP FOREIGN TABLE IF EXISTS customer_reviews_col ------ CREATE FOREIGN TABLE customer_reviews_col ( customer_id TEXT, review_date DATE, review_rating INTEGER, review_votes INTEGER, review_helpful_votes INTEGER, product_id CHAR(10), product_title TEXT, product_sales_rank BIGINT, product_group TEXT, product_category TEXT, product_subcategory TEXT, similar_product_ids CHAR(10)[] ) ------------- -- leave code below as is SERVER cstore_server OPTIONS(compression 'pglz');","title":"Notes from Udacity DEND for PostgreSQL"},{"location":"notes/postgresql/#notes-from-udacity-dend-for-postgresql","text":"","title":"Notes from Udacity DEND for PostgreSQL"},{"location":"notes/postgresql/#using-jyupter-notebook-to-run-sql","text":"You can use ipython-sql library. And, load it into the Jyupter notebook using %load_ext sql . To execute SQL queries, you write on of the following at the top of your cell. - %sql : - For a one-liner SQL query - You can access a python variable using $ - %%SQL : - For a multi-line SQL query - You can NOT access a python variable using $ Running a connection like: postgresql://postgres:postgres@db:5432/pagila to connect to the database.","title":"Using Jyupter Notebook to run SQL"},{"location":"notes/postgresql/#creating-a-database-and-fill-it-with-data","text":"Adding ! at the beginning at the Jyupter cell runs a command in shell. For example, we're not running python code but we are running the createdb and psql postgresql command-line utilities. An example: !PGPASSWORD=student createdb -h 127.0.0.1 -U student pagila !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-schema.sql !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-data.sql","title":"Creating a database and fill it with data"},{"location":"notes/postgresql/#connect-to-a-database","text":"Load the ipython-sql, create the connection variable, then connect to database. # Load ipython-sql library %load_ext sql # Connect to database DB_ENDPOINT = '127.0.0.1' DB = 'pagila' DB_USER = 'student' DB_PASSWORD = 'student' DB_PORT = '5432' # Creating the connection string and keep in python variable: conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, DB_PORT, DB) # Using the python variable %sql $conn_string","title":"Connect to a database"},{"location":"notes/postgresql/#retrieve-the-database-schema","text":"%%sql SELECT column_name, data_type FROM information_schema.columns WHERE table_name = \"dimdate';","title":"Retrieve the database schema"},{"location":"notes/postgresql/#create-fact-table-with-foreign-keys-using-references-constraint","text":"%%sql CREATE TABLE factSales ( sales_key SERIAL PRIMARY KEY, date_key integer REFERENCES dimdate (date_key), customer_key integer REFERENCES dimcustomer (customer_key), movie_key integer REFERENCES dimmovie (movie_key), store_key integer REFERENCES dimstore(store_key), sales_amount numeric );","title":"Create FACT table with foreign keys using REFERENCES constraint"},{"location":"notes/postgresql/#drop-tables-that-contain-foreign-keys","text":"You need to drop the table that has foreign keys references before you can drop other tables. %%sql DROP TABLE IF EXISTS factsales; DROP TABLE IF EXISTS dimdate; DROP TABLE IF EXISTS dimstore; DROP TABLE IF EXISTS dimmovie; DROP TABLE IF EXISTS dimcustomer;","title":"Drop tables that contain foreign keys"},{"location":"notes/postgresql/#etl-process-from-one-database-normalized-to-new-tables-star-schema","text":"An example to extract date data from a payment_date table, then transform and load into the dimDate table %%sql INSERT INTO dimDate (date_key, date, year, quarter, month, day, week, is_weekend) SELECT DISTINCT(TO_CHAR(payment_date :: DATE, 'yyyyMMDD')::integer) AS date_key, date(payment_date) AS date, EXTRACT(year FROM payment_date) AS year, EXTRACT(quarter FROM payment_date) AS quarter, EXTRACT(month FROM payment_date) AS month, EXTRACT(day FROM payment_date) AS day, EXTRACT(week FROM payment_date) AS week, CASE WHEN EXTRACT(ISODOW FROM payment_date) IN (6, 7) THEN true ELSE false END AS is_weekend FROM payment;","title":"ETL process from one database (Normalized) to new tables (Star Schema)"},{"location":"notes/postgresql/#check-the-performance-of-the-script","text":"Add %%time at the top of the execution block. An example, %%time %%sql SELECT dimMovie.title, dimDate.month, dimCustomer.city, sum(sales_amount) AS revenue FROM factSales JOIN dimMovie ON (dimMovie.movie_key = factSales.movie_key) JOIN dimDate ON (dimDate.date_key = factSales.date_key) JOIN dimCustomer ON (dimCustomer.customer_key = factSales.customer_key) GROUP BY (dimMovie.title, dimDate.month, dimCustomer.city) ORDER BY dimMovie.title, dimDate.month, dimCustomer.city, revenue DESC;","title":"Check the performance of the script"},{"location":"notes/postgresql/#using-columnar-storage-extension-in-postgresql","text":"The extension is cstore_fdw by citus_data: github link An example: %%sql -- load extension first time after install CREATE EXTENSION cstore_fdw; -- create server object CREATE SERVER cstore_server FOREIGN DATA WRAPPER cstore_fdw; -- create foreign table, i.e. customer reviews column DROP FOREIGN TABLE IF EXISTS customer_reviews_col ------ CREATE FOREIGN TABLE customer_reviews_col ( customer_id TEXT, review_date DATE, review_rating INTEGER, review_votes INTEGER, review_helpful_votes INTEGER, product_id CHAR(10), product_title TEXT, product_sales_rank BIGINT, product_group TEXT, product_category TEXT, product_subcategory TEXT, similar_product_ids CHAR(10)[] ) ------------- -- leave code below as is SERVER cstore_server OPTIONS(compression 'pglz');","title":"Using columnar storage extension in PostgreSQL"}]}