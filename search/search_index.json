{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pen my notes Personal notebook where I document my learnings and cheatsheets. For those who are reading this, kindly note that the documents can be very messy because I've created them in a quick and dirty manner.","title":"Pen my notes"},{"location":"#pen-my-notes","text":"Personal notebook where I document my learnings and cheatsheets. For those who are reading this, kindly note that the documents can be very messy because I've created them in a quick and dirty manner.","title":"Pen my notes"},{"location":"notes/aws-sdk/","text":"Using AWS SDK to automate Redshift cluster creation and deletion Notes taken from Udacity Data Engineering Nanodegree to automate the creation, and removal of Redshift Cluster. And, also setting up S3 bucket for storing and loading raw data. The following is for playing with Redshift database, and at the same time, not spending too much on it. Automating the creation and removal of the Redshift clusters has helped me saved a lot. To Begin There are few libraries needed - configparser and boto3 . I've also used Pandas library in the below example. Prepare the parameters and environment Create an external cfg file to store all the params, and load into python script using configparser . And, the following is how the config file looks like: [AWS] KEY=key SECRET=secret [DWH] DWH_CLUSTER_TYPE=multi-node DWH_NUM_NODES=4 DWH_NODE_TYPE=dc2.large DWH_IAM_ROLE_NAME=dwhRole DWH_CLUSTER_IDENTIFIER=dwhCluster DWH_DB=dwh DWH_DB_USER=dwhuser DWH_DB_PASSWORD=Passw0rd DWH_PORT=5439 Load the parameters into script The following demonstrate how to load the parameters into the notebook: import configparser import pandas as pd config = configparser.ConfigParser() config.read_file(open('dwh.cfg')) KEY = config.get('AWS','KEY') SECRET = config.get('AWS','SECRET') DWH_CLUSTER_TYPE = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\") DWH_NUM_NODES = config.get(\"DWH\",\"DWH_NUM_NODES\") DWH_NODE_TYPE = config.get(\"DWH\",\"DWH_NODE_TYPE\") DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\") DWH_DB = config.get(\"DWH\",\"DWH_DB\") DWH_DB_USER = config.get(\"DWH\",\"DWH_DB_USER\") DWH_DB_PASSWORD = config.get(\"DWH\",\"DWH_DB_PASSWORD\") DWH_PORT = config.get(\"DWH\",\"DWH_PORT\") DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\") (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB) pd.DataFrame({\"Param\": [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"], \"Value\": [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME] }) Set up the resources and clients for EC2, S3, IAM and Redshift Using boto3 library, create resources and clients in AWS. import boto3 ec2 = boto3.resource('ec2', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) s3 = boto3.resource('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) iam = boto3.client('iam', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) redshift = boto3.client('redshift', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) Test your connection to s3 bucket that store your data sources Try printing out the data sources from s3 using the following template: sampleDbBucket = s3.Bucket(\"bucket-name\") for obj in sampleDbBucket.objects.filter(Prefix=\"some prefix to filter data\"): print (obj) Create Role and Attach Policy Creating IAM role to allow AWS Redshift to call AWS services on your behalf. # Create Role try: print('1.1 Creating a new IAM Role') dwhRole = iam.create_role( Path='/', RoleName=DWH_IAM_ROLE_NAME, Description=\"Allows Redshift clusters to call AWS services on your behalf.\", AssumeRolePolicyDocument=json.dumps( { 'Statement': [{'Action': 'sts:AssumeRole', 'Effect': 'Allow', 'Principal': {'Service': 'redshift.amazonaws.com'}}], 'Version': '2012-10-17'} ) ) except Exception as e: print(e) # Attach Policy print('1.2 Attaching Policy') iam.attach_role_policy( RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" )['ResponseMetadata']['HTTPStatusCode'] # Get and print the IAM role ARN print('1.3 Get the IAM role ARN') roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn'] print(roleArn) Create Redshift Cluster Creating the redshift cluster using the standard template. You can check the script to automate this via carpark-sg-data-pipeline try: response = redshift.create_cluster( #HW ClusterType=DWH_CLUSTER_TYPE, NodeType=DWH_NODE_TYPE, NumberOfNodes=int(DWH_NUM_NODES), #Identifiers & Credentials DBName=DWH_DB, ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, MasterUsername=DWH_DB_USER, MasterUserPassword=DWH_DB_PASSWORD, #Roles (for s3 access) IamRoles=[roleArn] ) except Exception as e: print(e) Then, describe the cluster to examine the parameters and its status. Run this block several times until the cluster status becomes Available def prettyRedshiftProps(props): pd.set_option('display.max_colwidth', -1) keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId'] x = [(k, v) for k,v in props.items() if k in keysToShow] return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"]) myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] prettyRedshiftProps(myClusterProps) Print out the cluster endpoint. Only run this when the cluster is available DWH_ENDPOINT = myClusterProps['Endpoint']['Address'] DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn'] print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT) print(\"DWH_ROLE_ARN :: \", roleArn) Open an incoming TCP port to access the cluster endpoint try: vpc = ec2.Vpc(id=myClusterProps['VpcId']) defaultSg = list(vpc.security_groups.all())[0] print(defaultSg) defaultSg.authorize_ingress( GroupName=defaultSg.group_name, CidrIp='0.0.0.0/0', IpProtocol='TCP', FromPort=int(DWH_PORT), ToPort=int(DWH_PORT) ) except Exception as e: print(e) Test that you can access the cluster %load_ext sql conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB) print(conn_string) %sql $conn_string Remove Redshift Cluster Fetch Parameters print(\"1. Fetch params\") KEY, SECRET = get_config('access.cfg', 'AWS') DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME = get_config('dwh.cfg', 'DWH') Setup Resources and Clients # Setup resources and clients print(\"2. Setup Clients\") iam = boto3.client('iam',region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) redshift = boto3.client('redshift', region_name=\"us-west-2\", aws_access_key_id=KEY, aws_secret_access_key=SECRET) Delete the Redshift Clusters print(\"3. Deleting Redshift Clusters\") redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True) myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] while (myClusterProps['ClusterStatus'] == 'deleting'): print (\"Redshift is {}\".format(myClusterProps['ClusterStatus'])) sleep(60) try: myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] except Exception as e: print(e) break; print(\"Redshift is deleted\") Clean up all the resources print(\"4. Clean up Resources\") iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\") iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)","title":"Using AWS SDK to automate Redshift cluster creation and deletion"},{"location":"notes/aws-sdk/#using-aws-sdk-to-automate-redshift-cluster-creation-and-deletion","text":"Notes taken from Udacity Data Engineering Nanodegree to automate the creation, and removal of Redshift Cluster. And, also setting up S3 bucket for storing and loading raw data. The following is for playing with Redshift database, and at the same time, not spending too much on it. Automating the creation and removal of the Redshift clusters has helped me saved a lot.","title":"Using AWS SDK to automate Redshift cluster creation and deletion"},{"location":"notes/aws-sdk/#to-begin","text":"There are few libraries needed - configparser and boto3 . I've also used Pandas library in the below example.","title":"To Begin"},{"location":"notes/aws-sdk/#prepare-the-parameters-and-environment","text":"Create an external cfg file to store all the params, and load into python script using configparser . And, the following is how the config file looks like: [AWS] KEY=key SECRET=secret [DWH] DWH_CLUSTER_TYPE=multi-node DWH_NUM_NODES=4 DWH_NODE_TYPE=dc2.large DWH_IAM_ROLE_NAME=dwhRole DWH_CLUSTER_IDENTIFIER=dwhCluster DWH_DB=dwh DWH_DB_USER=dwhuser DWH_DB_PASSWORD=Passw0rd DWH_PORT=5439","title":"Prepare the parameters and environment"},{"location":"notes/aws-sdk/#load-the-parameters-into-script","text":"The following demonstrate how to load the parameters into the notebook: import configparser import pandas as pd config = configparser.ConfigParser() config.read_file(open('dwh.cfg')) KEY = config.get('AWS','KEY') SECRET = config.get('AWS','SECRET') DWH_CLUSTER_TYPE = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\") DWH_NUM_NODES = config.get(\"DWH\",\"DWH_NUM_NODES\") DWH_NODE_TYPE = config.get(\"DWH\",\"DWH_NODE_TYPE\") DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\") DWH_DB = config.get(\"DWH\",\"DWH_DB\") DWH_DB_USER = config.get(\"DWH\",\"DWH_DB_USER\") DWH_DB_PASSWORD = config.get(\"DWH\",\"DWH_DB_PASSWORD\") DWH_PORT = config.get(\"DWH\",\"DWH_PORT\") DWH_IAM_ROLE_NAME = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\") (DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB) pd.DataFrame({\"Param\": [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"], \"Value\": [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME] })","title":"Load the parameters into script"},{"location":"notes/aws-sdk/#set-up-the-resources-and-clients-for-ec2-s3-iam-and-redshift","text":"Using boto3 library, create resources and clients in AWS. import boto3 ec2 = boto3.resource('ec2', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) s3 = boto3.resource('s3', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) iam = boto3.client('iam', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) redshift = boto3.client('redshift', region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET)","title":"Set up the resources and clients for EC2, S3, IAM and Redshift"},{"location":"notes/aws-sdk/#test-your-connection-to-s3-bucket-that-store-your-data-sources","text":"Try printing out the data sources from s3 using the following template: sampleDbBucket = s3.Bucket(\"bucket-name\") for obj in sampleDbBucket.objects.filter(Prefix=\"some prefix to filter data\"): print (obj)","title":"Test your connection to s3 bucket that store your data sources"},{"location":"notes/aws-sdk/#create-role-and-attach-policy","text":"Creating IAM role to allow AWS Redshift to call AWS services on your behalf. # Create Role try: print('1.1 Creating a new IAM Role') dwhRole = iam.create_role( Path='/', RoleName=DWH_IAM_ROLE_NAME, Description=\"Allows Redshift clusters to call AWS services on your behalf.\", AssumeRolePolicyDocument=json.dumps( { 'Statement': [{'Action': 'sts:AssumeRole', 'Effect': 'Allow', 'Principal': {'Service': 'redshift.amazonaws.com'}}], 'Version': '2012-10-17'} ) ) except Exception as e: print(e) # Attach Policy print('1.2 Attaching Policy') iam.attach_role_policy( RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\" )['ResponseMetadata']['HTTPStatusCode'] # Get and print the IAM role ARN print('1.3 Get the IAM role ARN') roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn'] print(roleArn)","title":"Create Role and Attach Policy"},{"location":"notes/aws-sdk/#create-redshift-cluster","text":"Creating the redshift cluster using the standard template. You can check the script to automate this via carpark-sg-data-pipeline try: response = redshift.create_cluster( #HW ClusterType=DWH_CLUSTER_TYPE, NodeType=DWH_NODE_TYPE, NumberOfNodes=int(DWH_NUM_NODES), #Identifiers & Credentials DBName=DWH_DB, ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, MasterUsername=DWH_DB_USER, MasterUserPassword=DWH_DB_PASSWORD, #Roles (for s3 access) IamRoles=[roleArn] ) except Exception as e: print(e) Then, describe the cluster to examine the parameters and its status. Run this block several times until the cluster status becomes Available def prettyRedshiftProps(props): pd.set_option('display.max_colwidth', -1) keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId'] x = [(k, v) for k,v in props.items() if k in keysToShow] return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"]) myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] prettyRedshiftProps(myClusterProps) Print out the cluster endpoint. Only run this when the cluster is available DWH_ENDPOINT = myClusterProps['Endpoint']['Address'] DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn'] print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT) print(\"DWH_ROLE_ARN :: \", roleArn)","title":"Create Redshift Cluster"},{"location":"notes/aws-sdk/#open-an-incoming-tcp-port-to-access-the-cluster-endpoint","text":"try: vpc = ec2.Vpc(id=myClusterProps['VpcId']) defaultSg = list(vpc.security_groups.all())[0] print(defaultSg) defaultSg.authorize_ingress( GroupName=defaultSg.group_name, CidrIp='0.0.0.0/0', IpProtocol='TCP', FromPort=int(DWH_PORT), ToPort=int(DWH_PORT) ) except Exception as e: print(e)","title":"Open an incoming TCP port to access the cluster endpoint"},{"location":"notes/aws-sdk/#test-that-you-can-access-the-cluster","text":"%load_ext sql conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB) print(conn_string) %sql $conn_string","title":"Test that you can access the cluster"},{"location":"notes/aws-sdk/#remove-redshift-cluster","text":"","title":"Remove Redshift Cluster"},{"location":"notes/aws-sdk/#fetch-parameters","text":"print(\"1. Fetch params\") KEY, SECRET = get_config('access.cfg', 'AWS') DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME = get_config('dwh.cfg', 'DWH')","title":"Fetch Parameters"},{"location":"notes/aws-sdk/#setup-resources-and-clients","text":"# Setup resources and clients print(\"2. Setup Clients\") iam = boto3.client('iam',region_name='us-west-2', aws_access_key_id=KEY, aws_secret_access_key=SECRET) redshift = boto3.client('redshift', region_name=\"us-west-2\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)","title":"Setup Resources and Clients"},{"location":"notes/aws-sdk/#delete-the-redshift-clusters","text":"print(\"3. Deleting Redshift Clusters\") redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True) myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] while (myClusterProps['ClusterStatus'] == 'deleting'): print (\"Redshift is {}\".format(myClusterProps['ClusterStatus'])) sleep(60) try: myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0] except Exception as e: print(e) break; print(\"Redshift is deleted\")","title":"Delete the Redshift Clusters"},{"location":"notes/aws-sdk/#clean-up-all-the-resources","text":"print(\"4. Clean up Resources\") iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\") iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)","title":"Clean up all the resources"},{"location":"notes/cassandra/","text":"","title":"Cassandra"},{"location":"notes/docker-notes/","text":"Docker Notes Notes taken from reading how to use Docker online. The followings are resources I've read and used: Map a local folder/directory to the container Setting up PostgreSQL on Windows 10 with Docker Map a local folder/directory to the container The %cd% refers to the current directory/folder that you are in. And, map that to a directory inside the container. Recommendation is to navigate to the directory, then run the following command on Windows command prompt. ```command prompt docker run --rm -it -v %cd%:/data If you are using Windows Powershell, ```powershell docker run --rm -it -v ${PWD}:/data <container-name> And, if you are using Linux or Mac OS, docker run --rm -it -v `pwd`:/data <container-name> Setting up PostgreSQL on Windows 10 with Docker Setting up a container running Postgres and expose the port used by Postgres to allow access from the host. docker run -p 5432:5432 --name yourContainerName -e POSTGRES_PASSWORD=yourPassword -d postgres The problem using above approach is the risk of losing data, if rebuilding the container is necessary. Therefore, it is advisable to use a secondary container to hold the data, separating it from the Postgres container. The following command will create the data container using the Alpine image. You can read about this in the links under the Reference Material . docker create -v /var/lib/postgresql/data --name PostgresData alpine Important note: The -v parameter must match the path that Postgres expects. Then, you can set up a container that uses the data container as storage: docker run -p 5432:5432 --name yourContainerName -e POSTGRES_PASSWORD=yourPassword -d --volumes-from PostgresData postgres Removing the PostgreSQL container, including the associated volume Technically, your data will not be lost unless you use the -v flag when you remove the container. docker rm -v yourContainerName If you neglect to include the -v flag when removing the container, the data will remain in a dangling volume and, while still present on your host\u2019s file system, not very easy to work with and not automatically mounted when you start a new postgresql container. Potential Issues This is an error I received on Apr 13, 2019. You might receive error that says docker: Error response from daemon: driver failed programming external connectivity on endpoint ...: Error starting userland proxy: mkdir /port/tcp:0.0.0.0:5432:tcp:172.17.0.2:5432: input/output error. You need to quite/close the Docker Desktop, and restart the Docker Desktop. Then, it will work. Reference Material Setup PostgreSQL on Windows with Docker Dockerized Postgresql Development Environment Deploying PostgreSQL on a Docker Container Data Science at the command line","title":"Docker Notes"},{"location":"notes/docker-notes/#docker-notes","text":"Notes taken from reading how to use Docker online. The followings are resources I've read and used: Map a local folder/directory to the container Setting up PostgreSQL on Windows 10 with Docker","title":"Docker Notes"},{"location":"notes/docker-notes/#map-a-local-folderdirectory-to-the-container","text":"The %cd% refers to the current directory/folder that you are in. And, map that to a directory inside the container. Recommendation is to navigate to the directory, then run the following command on Windows command prompt. ```command prompt docker run --rm -it -v %cd%:/data If you are using Windows Powershell, ```powershell docker run --rm -it -v ${PWD}:/data <container-name> And, if you are using Linux or Mac OS, docker run --rm -it -v `pwd`:/data <container-name>","title":"Map a local folder/directory to the container"},{"location":"notes/docker-notes/#setting-up-postgresql-on-windows-10-with-docker","text":"Setting up a container running Postgres and expose the port used by Postgres to allow access from the host. docker run -p 5432:5432 --name yourContainerName -e POSTGRES_PASSWORD=yourPassword -d postgres The problem using above approach is the risk of losing data, if rebuilding the container is necessary. Therefore, it is advisable to use a secondary container to hold the data, separating it from the Postgres container. The following command will create the data container using the Alpine image. You can read about this in the links under the Reference Material . docker create -v /var/lib/postgresql/data --name PostgresData alpine Important note: The -v parameter must match the path that Postgres expects. Then, you can set up a container that uses the data container as storage: docker run -p 5432:5432 --name yourContainerName -e POSTGRES_PASSWORD=yourPassword -d --volumes-from PostgresData postgres","title":"Setting up PostgreSQL on Windows 10 with Docker"},{"location":"notes/docker-notes/#removing-the-postgresql-container-including-the-associated-volume","text":"Technically, your data will not be lost unless you use the -v flag when you remove the container. docker rm -v yourContainerName If you neglect to include the -v flag when removing the container, the data will remain in a dangling volume and, while still present on your host\u2019s file system, not very easy to work with and not automatically mounted when you start a new postgresql container.","title":"Removing the PostgreSQL container, including the associated volume"},{"location":"notes/docker-notes/#potential-issues","text":"This is an error I received on Apr 13, 2019. You might receive error that says docker: Error response from daemon: driver failed programming external connectivity on endpoint ...: Error starting userland proxy: mkdir /port/tcp:0.0.0.0:5432:tcp:172.17.0.2:5432: input/output error. You need to quite/close the Docker Desktop, and restart the Docker Desktop. Then, it will work.","title":"Potential Issues"},{"location":"notes/docker-notes/#reference-material","text":"Setup PostgreSQL on Windows with Docker Dockerized Postgresql Development Environment Deploying PostgreSQL on a Docker Container Data Science at the command line","title":"Reference Material"},{"location":"notes/git-notes/","text":"Git notes How to git push to an AWS EC2 remote using a PEM file How to deploy your project with a git push on your EC2 instance How to git push to an AWS EC2 remote using a PEM file I learn the following from alphacoder AWS provides a .pem file when creating EC2 instance. We can use this file to generate SSH keys for accessing the instance without using the PEM file. To begin, you should use Git Bash if you are using Windows 10, like I do Copy private key to .ssh folder $ cp /path/to/aws-ec2-instance.pem ~/.ssh/id_rsa_ec2 Generate and save public key Go to ~/.ssh folder, and run $ ssh-keygen -y -f aws-ec2-instance.pem > ~/.ssh/id_rsa_ec2.pub Add private key to ssh-agent Start ssh-agent $ eval \"$(ssh-agent -s)\" Then, add your key to the agent $ ssh-add ~/.ssh/id_rsa_ec2 After that you can access your instance without the PEM file $ ssh ubuntu@ec2-ip-address How to deploy your project with a git push on your EC2 instance I learn the following from kirankoduru.github.io Using the notes from the website, I can host codes on Github, then deploy the code to server by doing git push to EC2 instance. This makes the deployment fast and simple. To begin, you should setup the git push to AWS EC2 using PEM file Login to your EC2 machine through ssh $ ssh ubuntu@ec2-ip-address Setup a git bare repo This folder/repo will hold the code when you git push to EC2. For example, I'm creating a test project, and the bare git repo is test.git . And, in this example, I'm going to store my project in my home folder: /home/ubuntu/ sudo mkdir test.git cd test.git git init --bare Create a directory where you will deploy your project Go to the /home/ubuntu/ , and create the project folder sudo mkdir test Setup Git Hook After the above is done, go to the test.git folder, and create a file post-update using post-update.sample . cd test.git cd hooks mv post-update.sample post-update Then, edit the file vim post-update In the file, you should include the following: #!/bin/sh GIT_WORK_TREE=/home/ubuntu/test/ git checkout -f Setup the git remote after the above are completed Logout from the server, and go to your local git repo for project test, and add a remote git remote add production ubuntu@ec2-ip-address:/home/ubuntu/test.git Then, you are all done. You can push master branch to remote production and checkout the /home/ubuntu/test directory for the files. git push production master In case your push fails because of permission \u03bb git push production master Enumerating objects: 176, done. Counting objects: 100% (176/176), done. Delta compression using up to 4 threads Compressing objects: 100% (172/172), done. Writing objects: 100% (176/176), 660.67 KiB | 4.00 MiB/s, done. Total 176 (delta 89), reused 0 (delta 0) error: remote unpack failed: unable to create temporary object directory To ec2-ip-address:/home/ubuntu/test.git ! [remote rejected] master -> master (unpacker error) error: failed to push some refs to 'ubuntu@ec2-ip-address:/home/ubuntu/test.git' You can go to the folder in EC2 and give permission using chmod . I'm not very good at this yet, so I use 777 for the folder chmod -R 777 test.git","title":"Git notes"},{"location":"notes/git-notes/#git-notes","text":"How to git push to an AWS EC2 remote using a PEM file How to deploy your project with a git push on your EC2 instance","title":"Git notes"},{"location":"notes/git-notes/#how-to-git-push-to-an-aws-ec2-remote-using-a-pem-file","text":"I learn the following from alphacoder AWS provides a .pem file when creating EC2 instance. We can use this file to generate SSH keys for accessing the instance without using the PEM file. To begin, you should use Git Bash if you are using Windows 10, like I do","title":"How to git push to an AWS EC2 remote using a PEM file"},{"location":"notes/git-notes/#copy-private-key-to-ssh-folder","text":"$ cp /path/to/aws-ec2-instance.pem ~/.ssh/id_rsa_ec2","title":"Copy private key to .ssh folder"},{"location":"notes/git-notes/#generate-and-save-public-key","text":"Go to ~/.ssh folder, and run $ ssh-keygen -y -f aws-ec2-instance.pem > ~/.ssh/id_rsa_ec2.pub","title":"Generate and save public key"},{"location":"notes/git-notes/#add-private-key-to-ssh-agent","text":"Start ssh-agent $ eval \"$(ssh-agent -s)\" Then, add your key to the agent $ ssh-add ~/.ssh/id_rsa_ec2 After that you can access your instance without the PEM file $ ssh ubuntu@ec2-ip-address","title":"Add private key to ssh-agent"},{"location":"notes/git-notes/#how-to-deploy-your-project-with-a-git-push-on-your-ec2-instance","text":"I learn the following from kirankoduru.github.io Using the notes from the website, I can host codes on Github, then deploy the code to server by doing git push to EC2 instance. This makes the deployment fast and simple. To begin, you should setup the git push to AWS EC2 using PEM file","title":"How to deploy your project with a git push on your EC2 instance"},{"location":"notes/git-notes/#login-to-your-ec2-machine-through-ssh","text":"$ ssh ubuntu@ec2-ip-address","title":"Login to your EC2 machine through ssh"},{"location":"notes/git-notes/#setup-a-git-bare-repo","text":"This folder/repo will hold the code when you git push to EC2. For example, I'm creating a test project, and the bare git repo is test.git . And, in this example, I'm going to store my project in my home folder: /home/ubuntu/ sudo mkdir test.git cd test.git git init --bare","title":"Setup a git bare repo"},{"location":"notes/git-notes/#create-a-directory-where-you-will-deploy-your-project","text":"Go to the /home/ubuntu/ , and create the project folder sudo mkdir test","title":"Create a directory where you will deploy your project"},{"location":"notes/git-notes/#setup-git-hook","text":"After the above is done, go to the test.git folder, and create a file post-update using post-update.sample . cd test.git cd hooks mv post-update.sample post-update Then, edit the file vim post-update In the file, you should include the following: #!/bin/sh GIT_WORK_TREE=/home/ubuntu/test/ git checkout -f","title":"Setup Git Hook"},{"location":"notes/git-notes/#setup-the-git-remote-after-the-above-are-completed","text":"Logout from the server, and go to your local git repo for project test, and add a remote git remote add production ubuntu@ec2-ip-address:/home/ubuntu/test.git Then, you are all done. You can push master branch to remote production and checkout the /home/ubuntu/test directory for the files. git push production master","title":"Setup the git remote after the above are completed"},{"location":"notes/git-notes/#in-case-your-push-fails-because-of-permission","text":"\u03bb git push production master Enumerating objects: 176, done. Counting objects: 100% (176/176), done. Delta compression using up to 4 threads Compressing objects: 100% (172/172), done. Writing objects: 100% (176/176), 660.67 KiB | 4.00 MiB/s, done. Total 176 (delta 89), reused 0 (delta 0) error: remote unpack failed: unable to create temporary object directory To ec2-ip-address:/home/ubuntu/test.git ! [remote rejected] master -> master (unpacker error) error: failed to push some refs to 'ubuntu@ec2-ip-address:/home/ubuntu/test.git' You can go to the folder in EC2 and give permission using chmod . I'm not very good at this yet, so I use 777 for the folder chmod -R 777 test.git","title":"In case your push fails because of permission"},{"location":"notes/ipython/","text":"IPython Magic Commands in Juypter notebooks The following are notes taken while taking the Course - Data Engineering NanoDegree. Using IPython Magic commands to run SQL directly in Juypter notebooks Using ipython-sql library, and execute %load_ext sql to load the library in Juypter notebook. Using the following style at the top of each code cell to execute SQL queries: %sql : This is for executing one-liner SQL query. And, you can access a python variable using $ . %%sql : This is for executing multi-line SQL queries. But, you cannot access a python variable using $ Running a connection like: %sql postgresql://postgres:postgres@db:5432/pagila to connect to the database. Using ! to run commandline in Juypter notebooks Add ! at the beginning of the Juypter cell to run a commandline in shell. The following examples demonstrate how to run postgresql command-line utilities createdb and psql . Note: Postgres version of Pagila database can be found here !PGPASSWORD=student createdb -h 127.0.0.1 -U student pagila !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-schema.sql !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-data.sql Some examples of the IPython-magic commands with Juypter notebook and PostgreSQL database Connect to a database Load the ipython-sql library, create the connection variable, then connect to database. # Load ipython-sql library %load_ext sql # Connect to database DB_ENDPOINT = '127.0.0.1' DB = 'pagila' DB_USER = 'student' DB_PASSWORD = 'student' DB_PORT = '5432' # Creating the connection string and keep in python variable: conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, DB_PORT, DB) # Using the python variable %sql $conn_string Retrieve the database schema %%sql SELECT column_name, data_type FROM information_schema.columns WHERE table_name = \"dimdate'; Create FACT table with foreign keys using REFERENCES constraint %%sql CREATE TABLE factSales ( sales_key SERIAL PRIMARY KEY, date_key integer REFERENCES dimdate (date_key), customer_key integer REFERENCES dimcustomer (customer_key), movie_key integer REFERENCES dimmovie (movie_key), store_key integer REFERENCES dimstore(store_key), sales_amount numeric ); Drop tables that contain foreign keys You need to drop the table that has foreign keys references before you can drop other tables. %%sql DROP TABLE IF EXISTS factsales; DROP TABLE IF EXISTS dimdate; DROP TABLE IF EXISTS dimstore; DROP TABLE IF EXISTS dimmovie; DROP TABLE IF EXISTS dimcustomer; ETL process from one database (Normalized) to new tables (Star Schema) An example to extract date data from a payment_date table, then transform and load into the dimDate table %%sql INSERT INTO dimDate (date_key, date, year, quarter, month, day, week, is_weekend) SELECT DISTINCT(TO_CHAR(payment_date :: DATE, 'yyyyMMDD')::integer) AS date_key, date(payment_date) AS date, EXTRACT(year FROM payment_date) AS year, EXTRACT(quarter FROM payment_date) AS quarter, EXTRACT(month FROM payment_date) AS month, EXTRACT(day FROM payment_date) AS day, EXTRACT(week FROM payment_date) AS week, CASE WHEN EXTRACT(ISODOW FROM payment_date) IN (6, 7) THEN true ELSE false END AS is_weekend FROM payment; Check the performance of the script Add %%time at the top of the execution block. An example, %%time %%sql SELECT dimMovie.title, dimDate.month, dimCustomer.city, sum(sales_amount) AS revenue FROM factSales JOIN dimMovie ON (dimMovie.movie_key = factSales.movie_key) JOIN dimDate ON (dimDate.date_key = factSales.date_key) JOIN dimCustomer ON (dimCustomer.customer_key = factSales.customer_key) GROUP BY (dimMovie.title, dimDate.month, dimCustomer.city) ORDER BY dimMovie.title, dimDate.month, dimCustomer.city, revenue DESC;","title":"IPython Magic Commands in Juypter notebooks"},{"location":"notes/ipython/#ipython-magic-commands-in-juypter-notebooks","text":"The following are notes taken while taking the Course - Data Engineering NanoDegree.","title":"IPython Magic Commands in Juypter notebooks"},{"location":"notes/ipython/#using-ipython-magic-commands-to-run-sql-directly-in-juypter-notebooks","text":"Using ipython-sql library, and execute %load_ext sql to load the library in Juypter notebook. Using the following style at the top of each code cell to execute SQL queries: %sql : This is for executing one-liner SQL query. And, you can access a python variable using $ . %%sql : This is for executing multi-line SQL queries. But, you cannot access a python variable using $ Running a connection like: %sql postgresql://postgres:postgres@db:5432/pagila to connect to the database.","title":"Using IPython Magic commands to run SQL directly in Juypter notebooks"},{"location":"notes/ipython/#using-to-run-commandline-in-juypter-notebooks","text":"Add ! at the beginning of the Juypter cell to run a commandline in shell. The following examples demonstrate how to run postgresql command-line utilities createdb and psql . Note: Postgres version of Pagila database can be found here !PGPASSWORD=student createdb -h 127.0.0.1 -U student pagila !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-schema.sql !PGPASSWORD=student psql -q -h 127.0.0.1 -U student -d pagila -f Data/pagila-data.sql","title":"Using ! to run commandline in Juypter notebooks"},{"location":"notes/ipython/#some-examples-of-the-ipython-magic-commands-with-juypter-notebook-and-postgresql-database","text":"","title":"Some examples of the IPython-magic commands with Juypter notebook and PostgreSQL database"},{"location":"notes/ipython/#connect-to-a-database","text":"Load the ipython-sql library, create the connection variable, then connect to database. # Load ipython-sql library %load_ext sql # Connect to database DB_ENDPOINT = '127.0.0.1' DB = 'pagila' DB_USER = 'student' DB_PASSWORD = 'student' DB_PORT = '5432' # Creating the connection string and keep in python variable: conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, DB_PORT, DB) # Using the python variable %sql $conn_string","title":"Connect to a database"},{"location":"notes/ipython/#retrieve-the-database-schema","text":"%%sql SELECT column_name, data_type FROM information_schema.columns WHERE table_name = \"dimdate';","title":"Retrieve the database schema"},{"location":"notes/ipython/#create-fact-table-with-foreign-keys-using-references-constraint","text":"%%sql CREATE TABLE factSales ( sales_key SERIAL PRIMARY KEY, date_key integer REFERENCES dimdate (date_key), customer_key integer REFERENCES dimcustomer (customer_key), movie_key integer REFERENCES dimmovie (movie_key), store_key integer REFERENCES dimstore(store_key), sales_amount numeric );","title":"Create FACT table with foreign keys using REFERENCES constraint"},{"location":"notes/ipython/#drop-tables-that-contain-foreign-keys","text":"You need to drop the table that has foreign keys references before you can drop other tables. %%sql DROP TABLE IF EXISTS factsales; DROP TABLE IF EXISTS dimdate; DROP TABLE IF EXISTS dimstore; DROP TABLE IF EXISTS dimmovie; DROP TABLE IF EXISTS dimcustomer;","title":"Drop tables that contain foreign keys"},{"location":"notes/ipython/#etl-process-from-one-database-normalized-to-new-tables-star-schema","text":"An example to extract date data from a payment_date table, then transform and load into the dimDate table %%sql INSERT INTO dimDate (date_key, date, year, quarter, month, day, week, is_weekend) SELECT DISTINCT(TO_CHAR(payment_date :: DATE, 'yyyyMMDD')::integer) AS date_key, date(payment_date) AS date, EXTRACT(year FROM payment_date) AS year, EXTRACT(quarter FROM payment_date) AS quarter, EXTRACT(month FROM payment_date) AS month, EXTRACT(day FROM payment_date) AS day, EXTRACT(week FROM payment_date) AS week, CASE WHEN EXTRACT(ISODOW FROM payment_date) IN (6, 7) THEN true ELSE false END AS is_weekend FROM payment;","title":"ETL process from one database (Normalized) to new tables (Star Schema)"},{"location":"notes/ipython/#check-the-performance-of-the-script","text":"Add %%time at the top of the execution block. An example, %%time %%sql SELECT dimMovie.title, dimDate.month, dimCustomer.city, sum(sales_amount) AS revenue FROM factSales JOIN dimMovie ON (dimMovie.movie_key = factSales.movie_key) JOIN dimDate ON (dimDate.date_key = factSales.date_key) JOIN dimCustomer ON (dimCustomer.customer_key = factSales.customer_key) GROUP BY (dimMovie.title, dimDate.month, dimCustomer.city) ORDER BY dimMovie.title, dimDate.month, dimCustomer.city, revenue DESC;","title":"Check the performance of the script"},{"location":"notes/mkdocs-travis-gitpage/","text":"Notes about using Mkdocs, Docker, Travis and Github Pages Notes from learning how to create this note taking website hosted on Github Pages, using the Mkdocs. The deployment is automated using Docker and Travis. Check out the repo To begin Create a folder to store your notes. The folder structure should look like: note |_ docs (folder to store all your notes) |_ .gitignore |_ .travis.yml |_ mkdocs.yml |_ README.md In the .gitignore you should indicate git should ignore the folder site . Because, this is where your content will be built and published, and you shouldn't include it into your repo. \\site\\ In mkdocs.yml you can keep it simple in the beginning: theme: material In travis.yml We will be using squidfunk/mkdocs-material docker to help build the content. To know more about Mkdocs Material theme, you can check out the repo The below script will also use README.md as the index page for the site too. Running the docker build --clean will build the content into the docs\\site\\ folder. In deploy section, we will push the content in site to the gh-pages branch. cleanup need to set as true , else the published content will be deleted. os: linux language: minimal services: - docker before_deploy: - docker pull squidfunk/mkdocs-material - git branch gh-pages - cp README.md docs/index.md - docker run -ti -v `pwd`:/docs squidfunk/mkdocs-material build --clean - ls site deploy: provider: pages cleanup: true token: $GITHUB_TOKEN local_dir: site keep_history: true verbose: true on: branch: master","title":"Notes about using Mkdocs, Docker, Travis and Github Pages"},{"location":"notes/mkdocs-travis-gitpage/#notes-about-using-mkdocs-docker-travis-and-github-pages","text":"Notes from learning how to create this note taking website hosted on Github Pages, using the Mkdocs. The deployment is automated using Docker and Travis. Check out the repo","title":"Notes about using Mkdocs, Docker, Travis and Github Pages"},{"location":"notes/mkdocs-travis-gitpage/#to-begin","text":"Create a folder to store your notes. The folder structure should look like: note |_ docs (folder to store all your notes) |_ .gitignore |_ .travis.yml |_ mkdocs.yml |_ README.md","title":"To begin"},{"location":"notes/mkdocs-travis-gitpage/#in-the-gitignore","text":"you should indicate git should ignore the folder site . Because, this is where your content will be built and published, and you shouldn't include it into your repo. \\site\\","title":"In the .gitignore"},{"location":"notes/mkdocs-travis-gitpage/#in-mkdocsyml","text":"you can keep it simple in the beginning: theme: material","title":"In mkdocs.yml"},{"location":"notes/mkdocs-travis-gitpage/#in-travisyml","text":"We will be using squidfunk/mkdocs-material docker to help build the content. To know more about Mkdocs Material theme, you can check out the repo The below script will also use README.md as the index page for the site too. Running the docker build --clean will build the content into the docs\\site\\ folder. In deploy section, we will push the content in site to the gh-pages branch. cleanup need to set as true , else the published content will be deleted. os: linux language: minimal services: - docker before_deploy: - docker pull squidfunk/mkdocs-material - git branch gh-pages - cp README.md docs/index.md - docker run -ti -v `pwd`:/docs squidfunk/mkdocs-material build --clean - ls site deploy: provider: pages cleanup: true token: $GITHUB_TOKEN local_dir: site keep_history: true verbose: true on: branch: master","title":"In travis.yml"},{"location":"notes/odbc_w_r/","text":"Notes about ODBC drivers, and how to connect via R Notes taken while learning and using ODBC drivers to connect to databases in R Shiny apps during my daily work. Setting up ODBC drivers in Linux The following is how I setup ODBC drivers using AWS EC2, usually Ubuntu or Debian. Install necessary drivers To begin, install UnixODBC, which is required for all databases. Then, install common DB drivers. # Install the unixODBC library apt-get install unixodbc unixodbc-dev --install-suggests # SQL Server ODBC Drivers (Free TDS) apt-get install tdsodbc # PostgreSQL ODBC ODBC Drivers apt-get install odbc-postgresql # MySQL ODBC Drivers apt-get install libmyodbc # SQLite ODBC Drivers apt-get install libsqliteodbc Setup the db connections The following are the files for setting up the DSN information (DSN = Data Source Name). odbcinst.ini: define driver options odbc.ini: define connection options To find out where the configuration files are stored in your system: odbcinst -j Example of odbcinst.ini [PostgreSQL Driver] Driver = /usr/local/lib/psqlodbcw.so [SQLite Driver] Driver = /usr/local/lib/libsqlite3odbc.dylib Example of odbc.ini The driver 's name must match the name listed in the odbcinst.ini [PostgreSQL] Driver = PostgreSQL Driver Database = test_db Servername = localhost UserName = postgres Password = password Port = 5432 [SQLite] Driver = SQLite Driver Database = /tmp/testing Setting up ODBC drivers in Windows Open ODBC Data Source Administator (64 bit) tool Under User DSN, press Add . Select PostgreSQL Unicode(x64) . Fill up the database details, and select Test to test the connection to database. Connecting to Database in R Install odbc library install.packages(\"odbc\") Then, connect to the database using the DSN configuration files con <- DBI::dbConnect(odbc::odbc(), dsn=\"PostgreSQL\")","title":"Notes about ODBC drivers, and how to connect via R"},{"location":"notes/odbc_w_r/#notes-about-odbc-drivers-and-how-to-connect-via-r","text":"Notes taken while learning and using ODBC drivers to connect to databases in R Shiny apps during my daily work.","title":"Notes about ODBC drivers, and how to connect via R"},{"location":"notes/odbc_w_r/#setting-up-odbc-drivers-in-linux","text":"The following is how I setup ODBC drivers using AWS EC2, usually Ubuntu or Debian.","title":"Setting up ODBC drivers in Linux"},{"location":"notes/odbc_w_r/#install-necessary-drivers","text":"To begin, install UnixODBC, which is required for all databases. Then, install common DB drivers. # Install the unixODBC library apt-get install unixodbc unixodbc-dev --install-suggests # SQL Server ODBC Drivers (Free TDS) apt-get install tdsodbc # PostgreSQL ODBC ODBC Drivers apt-get install odbc-postgresql # MySQL ODBC Drivers apt-get install libmyodbc # SQLite ODBC Drivers apt-get install libsqliteodbc","title":"Install necessary drivers"},{"location":"notes/odbc_w_r/#setup-the-db-connections","text":"The following are the files for setting up the DSN information (DSN = Data Source Name). odbcinst.ini: define driver options odbc.ini: define connection options To find out where the configuration files are stored in your system: odbcinst -j Example of odbcinst.ini [PostgreSQL Driver] Driver = /usr/local/lib/psqlodbcw.so [SQLite Driver] Driver = /usr/local/lib/libsqlite3odbc.dylib Example of odbc.ini The driver 's name must match the name listed in the odbcinst.ini [PostgreSQL] Driver = PostgreSQL Driver Database = test_db Servername = localhost UserName = postgres Password = password Port = 5432 [SQLite] Driver = SQLite Driver Database = /tmp/testing","title":"Setup the db connections"},{"location":"notes/odbc_w_r/#setting-up-odbc-drivers-in-windows","text":"Open ODBC Data Source Administator (64 bit) tool Under User DSN, press Add . Select PostgreSQL Unicode(x64) . Fill up the database details, and select Test to test the connection to database.","title":"Setting up ODBC drivers in Windows"},{"location":"notes/odbc_w_r/#connecting-to-database-in-r","text":"Install odbc library install.packages(\"odbc\") Then, connect to the database using the DSN configuration files con <- DBI::dbConnect(odbc::odbc(), dsn=\"PostgreSQL\")","title":"Connecting to Database in R"},{"location":"notes/postgres/","text":"Notes about PostgreSQL database Some notes taken while learning and using PostgreSQL Database during Data Engineering Nanodegree, and daily work. Using columnar storage extension, cstore_fdw , in PostgreSQL The extension is cstore_fdw by citus_data : github link An example: %%sql -- load extension first time after install CREATE EXTENSION cstore_fdw; -- create server object CREATE SERVER cstore_server FOREIGN DATA WRAPPER cstore_fdw; -- create foreign table, i.e. customer reviews column DROP FOREIGN TABLE IF EXISTS customer_reviews_col ------ CREATE FOREIGN TABLE customer_reviews_col ( customer_id TEXT, review_date DATE, review_rating INTEGER, review_votes INTEGER, review_helpful_votes INTEGER, product_id CHAR(10), product_title TEXT, product_sales_rank BIGINT, product_group TEXT, product_category TEXT, product_subcategory TEXT, similar_product_ids CHAR(10)[] ) --------------------------------- -- leave code below as is SERVER cstore_server OPTIONS(compression 'pglz');","title":"Notes about PostgreSQL database"},{"location":"notes/postgres/#notes-about-postgresql-database","text":"Some notes taken while learning and using PostgreSQL Database during Data Engineering Nanodegree, and daily work.","title":"Notes about PostgreSQL database"},{"location":"notes/postgres/#using-columnar-storage-extension-cstore_fdw-in-postgresql","text":"The extension is cstore_fdw by citus_data : github link An example: %%sql -- load extension first time after install CREATE EXTENSION cstore_fdw; -- create server object CREATE SERVER cstore_server FOREIGN DATA WRAPPER cstore_fdw; -- create foreign table, i.e. customer reviews column DROP FOREIGN TABLE IF EXISTS customer_reviews_col ------ CREATE FOREIGN TABLE customer_reviews_col ( customer_id TEXT, review_date DATE, review_rating INTEGER, review_votes INTEGER, review_helpful_votes INTEGER, product_id CHAR(10), product_title TEXT, product_sales_rank BIGINT, product_group TEXT, product_category TEXT, product_subcategory TEXT, similar_product_ids CHAR(10)[] ) --------------------------------- -- leave code below as is SERVER cstore_server OPTIONS(compression 'pglz');","title":"Using columnar storage extension, cstore_fdw, in PostgreSQL"},{"location":"notes/redshift-dwh/","text":"Notes on Data Warehouse Notes taken while taking the Udacity Data Engineering Nanodegree. ETL architecture Data Ingestion Ingestion should be done at scale using the COPY command, because inserting row by row is too slow. If the file is huge, Splitting the file into multiple files Ingest in parallel Using common prefix, or Using a manifest file Other consideration Better to ingest from the same AWS region Better to compress all the CSV files Using Redshift - Optimizing the table design Redshift clusters have multiple nodes, so you can optimize the design of the table to ingest your data and improve its performance. When a table is partitioned up into many pieces and distributed across slices in different machines, this is done blindly. If one has an idea about the frequent access pattern of a table, one can choose a cleverer strategy. There are 2 possible strategies: Distribution Style Sorting Key Distribution style EVEN distribution With an EVEN key distribution style, a table is partitioned on slices such that each slice would have an almost equal number of records from the partitioned table. (round-robin method) Joining 2 tables distributed using an EVEN stategy is slow because records will be shuffled for putting together the join result. ALL distribution Small tables could be replicated on all slices to speed up joins. And, these are used frequently for dimension tables. (a.k.a broadcasting) AUTO distribution This strategy leave the decision to Redshift to decide. In general, this is what they do: \"Small enough\" tables are distributed with an ALL distribution strategy, while Large tables are distributed with EVEN strategy. KEY distribution Rows with similar keys are put in the same slice. But, this can lead to a skewed distribution, if some dist keys are more frequent than others. However, when a dimension table is too huge to be distributed across ALL strategy, one can organize the distribution for both fact table and dimension table using the same dist key. (Useful trick) Therefore, Redshift collocates the rows from both table one the same slice, using the joining keys. Sorting Key One can define its columns as sort key. Then, upon loading, rows are sorted before distribution to slices. This minimizes the query time since each node already has contiguous ranges of rows based on the sorting key. This is useful for columns that are frequently used in sorting like date dimension (SQL query by ) and, its corresponding foreign key in the fact table.","title":"Notes on Data Warehouse"},{"location":"notes/redshift-dwh/#notes-on-data-warehouse","text":"Notes taken while taking the Udacity Data Engineering Nanodegree.","title":"Notes on Data Warehouse"},{"location":"notes/redshift-dwh/#etl-architecture","text":"","title":"ETL architecture"},{"location":"notes/redshift-dwh/#data-ingestion","text":"Ingestion should be done at scale using the COPY command, because inserting row by row is too slow. If the file is huge, Splitting the file into multiple files Ingest in parallel Using common prefix, or Using a manifest file Other consideration Better to ingest from the same AWS region Better to compress all the CSV files","title":"Data Ingestion"},{"location":"notes/redshift-dwh/#using-redshift-optimizing-the-table-design","text":"Redshift clusters have multiple nodes, so you can optimize the design of the table to ingest your data and improve its performance. When a table is partitioned up into many pieces and distributed across slices in different machines, this is done blindly. If one has an idea about the frequent access pattern of a table, one can choose a cleverer strategy. There are 2 possible strategies: Distribution Style Sorting Key","title":"Using Redshift - Optimizing the table design"},{"location":"notes/redshift-dwh/#distribution-style","text":"EVEN distribution With an EVEN key distribution style, a table is partitioned on slices such that each slice would have an almost equal number of records from the partitioned table. (round-robin method) Joining 2 tables distributed using an EVEN stategy is slow because records will be shuffled for putting together the join result. ALL distribution Small tables could be replicated on all slices to speed up joins. And, these are used frequently for dimension tables. (a.k.a broadcasting) AUTO distribution This strategy leave the decision to Redshift to decide. In general, this is what they do: \"Small enough\" tables are distributed with an ALL distribution strategy, while Large tables are distributed with EVEN strategy. KEY distribution Rows with similar keys are put in the same slice. But, this can lead to a skewed distribution, if some dist keys are more frequent than others. However, when a dimension table is too huge to be distributed across ALL strategy, one can organize the distribution for both fact table and dimension table using the same dist key. (Useful trick) Therefore, Redshift collocates the rows from both table one the same slice, using the joining keys. Sorting Key One can define its columns as sort key. Then, upon loading, rows are sorted before distribution to slices. This minimizes the query time since each node already has contiguous ranges of rows based on the sorting key. This is useful for columns that are frequently used in sorting like date dimension (SQL query by ) and, its corresponding foreign key in the fact table.","title":"Distribution style"},{"location":"r-shiny-resource/","text":"R and Shiny Resources List There are already too many out there, and it can be difficult to scan through the starred list to find them. So, I'm just compiling a list linking to all these awesome lists. I might add some of my favourite list at the bottom, to have faster access to them. Shiny's List Awesome Shiny lists from Github HTML Widgets R's List Awesome R lists from Github Shiny's list The following is a list of useful resources for building Shiny Apps. Awesome Shiny lists from Github Awesome Shiny Extension by Nanxstats : From this list, you can find curated list of packages (Generic and Dashboard theming, UI and Visualization components, Backend components like DB, Notification, Job scheduling and lastly, developer tools) that offer extended UI or Server components for the R Web Framework, Shiny Awesome R Shiny by Grabear : From this list, you can find curated list of tutorials, packages that extends UI and Server components, as well as Shiny Apps examples created by others. Awesome Shiny Apps for Statistics : A curated list of Shiny Apps for statistics that can help self-learners to visualize statistics concept . HTML Widgets billboarder package by dreamRs : This package allows you to use billboard.js based on D3 v4+ Shiny JS Tutorials : tutorials materials on R/Shiny bindings to popular javascript libraries. One demo: Friss Analytics where the dashboard is used by an insurance company that screens people who apply for a new insurance during underwriting. Underwriting is the process in which an insurance company assesses whether or not it should accept a person into their portfolio. If the risk for specific type of claims is deemed too high, the insurer may decide to reject an application. https://github.com/Appsilon/shiny.semantic, https://github.com/Appsilon/semantic.dashboard/ Books Mastering Shiny Shiny Apps built by others Shiny-app-spotify : Spotify Personalized Playlist Recommendation through ML and Data Visualization. Tutorials on accessing spotify data can be found from RCharlie's blog post Radiant - Business analytics using R and Shiny : Radiant is an open-source platform-independent browser-based interface for business analytics in R. The application is based on the Shiny package and can be run locally or on a server. You can check out the video on the website I love Ikea Shiny App : One of my favourite apps Real Estate Market Forecasting and Analytics Automated Forecasting with Shiny R's list The following is a list of useful resources for writing R applications Awesome R lists from Github Awesome R : Quite a complete list of popular R packages for many things from Data Manipulation, Parallel Computing, Language API, Database Management, Machine Learning, NLP, Bayesian, Optimization and etc.. A more web browser friendly website can be found in https://awesome-r.com/index.html","title":"R and Shiny Resources List"},{"location":"r-shiny-resource/#r-and-shiny-resources-list","text":"There are already too many out there, and it can be difficult to scan through the starred list to find them. So, I'm just compiling a list linking to all these awesome lists. I might add some of my favourite list at the bottom, to have faster access to them. Shiny's List Awesome Shiny lists from Github HTML Widgets R's List Awesome R lists from Github","title":"R and Shiny Resources List"},{"location":"r-shiny-resource/#shinys-list","text":"The following is a list of useful resources for building Shiny Apps.","title":"Shiny's list"},{"location":"r-shiny-resource/#awesome-shiny-lists-from-github","text":"Awesome Shiny Extension by Nanxstats : From this list, you can find curated list of packages (Generic and Dashboard theming, UI and Visualization components, Backend components like DB, Notification, Job scheduling and lastly, developer tools) that offer extended UI or Server components for the R Web Framework, Shiny Awesome R Shiny by Grabear : From this list, you can find curated list of tutorials, packages that extends UI and Server components, as well as Shiny Apps examples created by others. Awesome Shiny Apps for Statistics : A curated list of Shiny Apps for statistics that can help self-learners to visualize statistics concept .","title":"Awesome Shiny lists from Github"},{"location":"r-shiny-resource/#html-widgets","text":"billboarder package by dreamRs : This package allows you to use billboard.js based on D3 v4+ Shiny JS Tutorials : tutorials materials on R/Shiny bindings to popular javascript libraries. One demo: Friss Analytics where the dashboard is used by an insurance company that screens people who apply for a new insurance during underwriting. Underwriting is the process in which an insurance company assesses whether or not it should accept a person into their portfolio. If the risk for specific type of claims is deemed too high, the insurer may decide to reject an application. https://github.com/Appsilon/shiny.semantic, https://github.com/Appsilon/semantic.dashboard/","title":"HTML Widgets"},{"location":"r-shiny-resource/#books","text":"Mastering Shiny","title":"Books"},{"location":"r-shiny-resource/#shiny-apps-built-by-others","text":"Shiny-app-spotify : Spotify Personalized Playlist Recommendation through ML and Data Visualization. Tutorials on accessing spotify data can be found from RCharlie's blog post Radiant - Business analytics using R and Shiny : Radiant is an open-source platform-independent browser-based interface for business analytics in R. The application is based on the Shiny package and can be run locally or on a server. You can check out the video on the website I love Ikea Shiny App : One of my favourite apps Real Estate Market Forecasting and Analytics Automated Forecasting with Shiny","title":"Shiny Apps built by others"},{"location":"r-shiny-resource/#rs-list","text":"The following is a list of useful resources for writing R applications","title":"R's list"},{"location":"r-shiny-resource/#awesome-r-lists-from-github","text":"Awesome R : Quite a complete list of popular R packages for many things from Data Manipulation, Parallel Computing, Language API, Database Management, Machine Learning, NLP, Bayesian, Optimization and etc.. A more web browser friendly website can be found in https://awesome-r.com/index.html","title":"Awesome R lists from Github"},{"location":"r-shiny-resource/r-notes/","text":"R notes Knit RMD files via commandline Knit RMD files via commandline The command to knit via Rstudio is rmarkdown::render('test.Rmd', 'html_document') If you have added a specific format in the YAML, that you wish to knit into, you can just omit the html_document . rmarkdown::render(\"test.Rmd\") Then, for command line command: Rscript -e 'library(rmarkdown);rmarkdown::render(\"/path/to/test.Rmd\")' Note: If you face issues with pandoc version 1.12.3 or higher is required and was not found , you can try the solution from here . I basically use export RSTUDIO_PANDOC=\"/c/Program Files/RStudio/bin/pandoc/\"","title":"R notes"},{"location":"r-shiny-resource/r-notes/#r-notes","text":"Knit RMD files via commandline","title":"R notes"},{"location":"r-shiny-resource/r-notes/#knit-rmd-files-via-commandline","text":"The command to knit via Rstudio is rmarkdown::render('test.Rmd', 'html_document') If you have added a specific format in the YAML, that you wish to knit into, you can just omit the html_document . rmarkdown::render(\"test.Rmd\") Then, for command line command: Rscript -e 'library(rmarkdown);rmarkdown::render(\"/path/to/test.Rmd\")' Note: If you face issues with pandoc version 1.12.3 or higher is required and was not found , you can try the solution from here . I basically use export RSTUDIO_PANDOC=\"/c/Program Files/RStudio/bin/pandoc/\"","title":"Knit RMD files via commandline"},{"location":"woocommerce-setup-notes/woocommerce-notes/","text":"WooCommerce Setup Notes WooCommerce Setup Notes Set Up Automated Order Notification Emails with Mailchimp Set Up Pop-up Dialog with Mailchimp in Wordpress to encourage sign-up and receive discount codes Create Discount Code In WooCommerce Set Up Automated Order Notification Emails with Mailchimp To begin, you need to have a Mailchimp account. Then, from the Wordpress/WooCommerce, install the plugin - Mailchimp for WooCommerce (install the latest version). Log into Mailchimp . Select Automate > Email . Go to E-Commerce tab, and select Enable Order Notifications . Choose the type of emails you wish to automate, and design them. After that, select continue and start the automation. Log into Wordpress , go to WooCommerce section. Go to Settings > Emails tab. Disable those emails that you're automating in WooCommerce. In my case, Processing order, Complete order, Cancel order and etc.. Note: One challenge with Mailchimp's automated emails is you're unable to add shipment tracking code to the email automatically. Another enhancement that I'd would love to see is adding some personalization in email, as well as changing the text of the button in the email. Set Up Pop-up Dialog with Mailchimp in Wordpress to encourage sign-up and receive discount codes To begin, you need to have a Mailchimp account, and also an audience list. Log into Mailchimp . Select Audience from the top navigation bar. Select the Manage Audience drop-down menu. And, choose Signup forms . Select Subscriber Pop-up . Then, follow the wizard to create your email design. If you wish to include a discount code, you should create the code in WooCommerce , and add it to the email. Under the Design > Pop-up settings , I usually set Display pop-up After 20 seconds . With API 3.0, you can follow the wizard to the end, and it will automatically enabled to run. Create Discount Code In WooCommerce Log into Wordpress , go to WooCommerce section. Go to Coupons . Then, follow the wizard to create your discount code. Additional Notes: Under General tab, you can set discount type to be in percentage or fixed amount . Then, you can set the amount in Coupon Amount . You don't need to include percentage or dollar sign. Set an expiry date for the discount in Coupon Expiry Date . For new subscriber discount, you can leave it blank. Under Usage Restriction tab, It is important to check the checkbox - Individual Use only . This ensure that the code cannot be used together with other discount codes. If you want to restrict the discount code to be for those subscribers, you can put those emails in Allowed Emails . Under Usage Limits tab, You can set the number of discount code to be used. For example, you can issue the discount code to be used only 100 times, then it will disabled. You can set how many times each user can use the discount code. This is tied to his/her email address. For example, welcome discount should only be used once per subscriber. Shipment Tracking plugin for WooCommerce You need to pay a high fee to use WooCommerce Shipment Tracking plugin. Therefore, I recommend that you use Aftership plugin instead. After installing, you should enable the tracking from the Aftership > settings . Thereafter, you will be able to insert tracking code into the orders. Your customers will be able to check the tracking code from the Account page in Wordpress/WooCommerce. Unfortunately, the tracking code and link will not appear in the order email. This makes it inconvenient for the customers to track their order.","title":"WooCommerce Setup Notes"},{"location":"woocommerce-setup-notes/woocommerce-notes/#woocommerce-setup-notes","text":"WooCommerce Setup Notes Set Up Automated Order Notification Emails with Mailchimp Set Up Pop-up Dialog with Mailchimp in Wordpress to encourage sign-up and receive discount codes Create Discount Code In WooCommerce","title":"WooCommerce Setup Notes"},{"location":"woocommerce-setup-notes/woocommerce-notes/#set-up-automated-order-notification-emails-with-mailchimp","text":"To begin, you need to have a Mailchimp account. Then, from the Wordpress/WooCommerce, install the plugin - Mailchimp for WooCommerce (install the latest version). Log into Mailchimp . Select Automate > Email . Go to E-Commerce tab, and select Enable Order Notifications . Choose the type of emails you wish to automate, and design them. After that, select continue and start the automation. Log into Wordpress , go to WooCommerce section. Go to Settings > Emails tab. Disable those emails that you're automating in WooCommerce. In my case, Processing order, Complete order, Cancel order and etc.. Note: One challenge with Mailchimp's automated emails is you're unable to add shipment tracking code to the email automatically. Another enhancement that I'd would love to see is adding some personalization in email, as well as changing the text of the button in the email.","title":"Set Up Automated Order Notification Emails with Mailchimp"},{"location":"woocommerce-setup-notes/woocommerce-notes/#set-up-pop-up-dialog-with-mailchimp-in-wordpress-to-encourage-sign-up-and-receive-discount-codes","text":"To begin, you need to have a Mailchimp account, and also an audience list. Log into Mailchimp . Select Audience from the top navigation bar. Select the Manage Audience drop-down menu. And, choose Signup forms . Select Subscriber Pop-up . Then, follow the wizard to create your email design. If you wish to include a discount code, you should create the code in WooCommerce , and add it to the email. Under the Design > Pop-up settings , I usually set Display pop-up After 20 seconds . With API 3.0, you can follow the wizard to the end, and it will automatically enabled to run.","title":"Set Up Pop-up Dialog with Mailchimp in Wordpress to encourage sign-up and receive discount codes"},{"location":"woocommerce-setup-notes/woocommerce-notes/#create-discount-code-in-woocommerce","text":"Log into Wordpress , go to WooCommerce section. Go to Coupons . Then, follow the wizard to create your discount code. Additional Notes: Under General tab, you can set discount type to be in percentage or fixed amount . Then, you can set the amount in Coupon Amount . You don't need to include percentage or dollar sign. Set an expiry date for the discount in Coupon Expiry Date . For new subscriber discount, you can leave it blank. Under Usage Restriction tab, It is important to check the checkbox - Individual Use only . This ensure that the code cannot be used together with other discount codes. If you want to restrict the discount code to be for those subscribers, you can put those emails in Allowed Emails . Under Usage Limits tab, You can set the number of discount code to be used. For example, you can issue the discount code to be used only 100 times, then it will disabled. You can set how many times each user can use the discount code. This is tied to his/her email address. For example, welcome discount should only be used once per subscriber.","title":"Create Discount Code In WooCommerce"},{"location":"woocommerce-setup-notes/woocommerce-notes/#shipment-tracking-plugin-for-woocommerce","text":"You need to pay a high fee to use WooCommerce Shipment Tracking plugin. Therefore, I recommend that you use Aftership plugin instead. After installing, you should enable the tracking from the Aftership > settings . Thereafter, you will be able to insert tracking code into the orders. Your customers will be able to check the tracking code from the Account page in Wordpress/WooCommerce. Unfortunately, the tracking code and link will not appear in the order email. This makes it inconvenient for the customers to track their order.","title":"Shipment Tracking plugin for WooCommerce"},{"location":"woocommerce-setup-notes/wordpress-notes/","text":"Wordpress Configuration Some plugins Wordfence Security by Wordfence: Firewall, Scanning features for malwares, block IP addresses. Loginizer Security: limit number of times users can retries their failed logins. And, block those failed more than 3 times, and adding them to blacklist. WooCommerce by Automattic WooCommerce Admin WooCommerce Paypal Checkout Gateway WooCommerce Stripe Gateway WooCommerce Services Mailchimp for Woocommerce by Mailchimp","title":"Wordpress Configuration"},{"location":"woocommerce-setup-notes/wordpress-notes/#wordpress-configuration","text":"","title":"Wordpress Configuration"},{"location":"woocommerce-setup-notes/wordpress-notes/#some-plugins","text":"Wordfence Security by Wordfence: Firewall, Scanning features for malwares, block IP addresses. Loginizer Security: limit number of times users can retries their failed logins. And, block those failed more than 3 times, and adding them to blacklist. WooCommerce by Automattic WooCommerce Admin WooCommerce Paypal Checkout Gateway WooCommerce Stripe Gateway WooCommerce Services Mailchimp for Woocommerce by Mailchimp","title":"Some plugins"}]}